{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gsMrWT3u4WM"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Install the intersection example.\n",
        "\n",
        "**Note**: The runtime needs to be restarted after installing the dependencies, hence `os.kill(os.getpid(), 9)` is added to stop the current session. Please ignore any resulting error message and simply continue to execute the subsequent cells as per normal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "!git clone https://github.com/huawei-noah/SMARTS /content/SMARTS\n",
        "!cd /content/SMARTS && git checkout intersection-v0 && cd /content/SMARTS/examples/rl/intersection && pip install --force-reinstall .\n",
        "import sys\n",
        "sys.path.insert(0, \"/content/SMARTS/\")\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Develop reinforcement learning code\n",
        "\n",
        "We begin by building the necessary environment wrappers. Firstly, an info wrapper is built to help log instances when the ego agent succesfully completes an unprotected left turn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Any, Dict, Tuple\n",
        "\n",
        "import gym\n",
        "\n",
        "\n",
        "class Info(gym.Wrapper):\n",
        "    def __init__(self, env: gym.Env):\n",
        "        super(Info, self).__init__(env)\n",
        "\n",
        "    def step(self, action: Any) -> Tuple[Any, float, bool, Dict[str, Any]]:\n",
        "        \"\"\"Steps the environment. A new \"is_success\" key is added to the\n",
        "        returned `info`.\n",
        "\n",
        "        Args:\n",
        "            action (Any): Action for the agent.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[ Any, float, bool, Dict[str, Any] ]:\n",
        "                Observation, reward, done, and info, for the agent is returned.\n",
        "        \"\"\"\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        info[\"is_success\"] = bool(info[\"score\"])\n",
        "\n",
        "        return obs, reward, done, info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `intersection-v0` environment has a continuous action space of `gym.spaces.Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32)` described by\n",
        "+ Throttle: [0,1]\n",
        "+ Brake: [0,1]\n",
        "+ Steering: [-1,1]\n",
        "\n",
        "In order to build a simple reinforcement learning policy, we discretise the action space using an action wrapper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Callable, Tuple\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Action(gym.ActionWrapper):\n",
        "    def __init__(self, env: gym.Env):\n",
        "        super().__init__(env)\n",
        "        self._wrapper, self.action_space = _discrete()\n",
        "\n",
        "    def action(self, action):\n",
        "        \"\"\"Adapts the action input to the wrapped environment.\n",
        "\n",
        "        Note: Users should not directly call this method.\n",
        "        \"\"\"\n",
        "        wrapped_act = self._wrapper(action)\n",
        "        return wrapped_act\n",
        "\n",
        "\n",
        "def _discrete() -> Tuple[Callable[[int], np.ndarray], gym.Space]:\n",
        "    space = gym.spaces.Discrete(n=4)\n",
        "\n",
        "    action_map = {\n",
        "        # key: [throttle, brake, steering]\n",
        "        0: [0.3, 0, 0],  # keep_direction\n",
        "        1: [0, 1, 0],  # slow_down\n",
        "        2: [0.3, 0, -0.5],  # turn_left\n",
        "        3: [0.3, 0, 0.5],  # turn_right\n",
        "    }\n",
        "\n",
        "    def wrapper(model_action: int) -> np.ndarray:\n",
        "        throttle, brake, steering = action_map[model_action]\n",
        "        return np.array([throttle, brake, steering], dtype=np.float32)\n",
        "\n",
        "    return wrapper, space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Next, we define the rewards using a reward wrapper. The agent is rewarded based on the distance travelled (in meters) per step and is penalised when it collides, goes off-road, goes off-route, goes wrong-way, or drives on the road shoulder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "from typing import Dict\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Reward(gym.Wrapper):\n",
        "    def __init__(self, env: gym.Env):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Adapts the wrapped environment's step.\n",
        "\n",
        "        Note: Users should not directly call this method.\n",
        "        \"\"\"\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        wrapped_reward = self._reward(obs, reward)\n",
        "\n",
        "        if done:\n",
        "            if obs[\"events\"][\"reached_goal\"]:\n",
        "                print(f\"ENV: Hooray! Vehicle reached goal.\")\n",
        "            elif obs[\"events\"][\"reached_max_episode_steps\"]:\n",
        "                print(f\"ENV: Vehicle reached max episode steps.\")\n",
        "            elif (\n",
        "                obs[\"events\"][\"off_road\"]\n",
        "                | obs[\"events\"][\"collisions\"]\n",
        "                | obs[\"events\"][\"off_route\"]\n",
        "                | obs[\"events\"][\"on_shoulder\"]\n",
        "                | obs[\"events\"][\"wrong_way\"]\n",
        "            ):\n",
        "                pass\n",
        "            else:\n",
        "                print(\"Events: \", obs[\"events\"])\n",
        "                raise Exception(\"Episode ended for unknown reason.\")\n",
        "\n",
        "        return obs, wrapped_reward, done, info\n",
        "\n",
        "    def _reward(self, obs: Dict[str, gym.Space], env_reward: np.float64) -> np.float64:\n",
        "        reward = 0\n",
        "\n",
        "        # Penalty for driving off road\n",
        "        if obs[\"events\"][\"off_road\"]:\n",
        "            reward -= 10\n",
        "            print(f\"ENV: Vehicle went off road.\")\n",
        "            return np.float64(reward)\n",
        "\n",
        "        # Penalty for driving on road shoulder\n",
        "        if obs[\"events\"][\"on_shoulder\"]:\n",
        "            reward -= 10\n",
        "            print(f\"ENV: Vehicle went on road shoulder.\")\n",
        "            return np.float64(reward)\n",
        "\n",
        "        # Penalty for driving on wrong way\n",
        "        if obs[\"events\"][\"wrong_way\"]:\n",
        "            reward -= 10\n",
        "            print(f\"ENV: Vehicle went wrong way.\")\n",
        "            return np.float64(reward)\n",
        "\n",
        "        # Penalty for colliding\n",
        "        if obs[\"events\"][\"collisions\"]:\n",
        "            reward -= 10\n",
        "            print(f\"ENV: Vehicle collided.\")\n",
        "            return np.float64(reward)\n",
        "\n",
        "        # Penalty for driving off route\n",
        "        if obs[\"events\"][\"off_route\"]:\n",
        "            reward -= 10\n",
        "            print(f\"ENV: Vehicle went off route.\")\n",
        "            return np.float64(reward)\n",
        "\n",
        "        # Reward for distance travelled\n",
        "        reward += env_reward\n",
        "\n",
        "        return np.float64(reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "The observation space of `intersection-v0` environment is described in `SMARTS/smarts/env/intersection_env.py`. \n",
        "\n",
        "In this tutorial, only the top-down rgb image from the observation space is used as input to our reinforcement learning policy. Therefore, a wrapper is built to filter the top-down rgb image observation.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "from typing import Dict\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Observation(gym.Wrapper):\n",
        "    def __init__(self, env: gym.Env):\n",
        "        super().__init__(env)\n",
        "        old_space = env.observation_space[\"rgb\"]\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=255,\n",
        "            shape=(old_space.shape[-1],) + old_space.shape[:-1],\n",
        "            dtype=np.uint8,\n",
        "        )\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Steps the environment by one step.\n",
        "\n",
        "        Args:\n",
        "            actions (Any): Agent's action.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[ np.ndarray, float, bool, Dict[str, Any] ]:\n",
        "                Observation, reward, done, info, of the agent.\n",
        "        \"\"\"\n",
        "        obs, rewards, dones, infos = self.env.step(action)\n",
        "        filtered = filter_obs(obs)\n",
        "        return filtered, rewards, dones, infos\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the environment.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Agent's observation after reset.\n",
        "        \"\"\"\n",
        "        obs = self.env.reset()\n",
        "        filtered = filter_obs(obs)\n",
        "        return filtered\n",
        "\n",
        "\n",
        "def filter_obs(obs: Dict[str, gym.Space]) -> np.ndarray:\n",
        "    rgb = obs[\"rgb\"]\n",
        "\n",
        "    # Channel first\n",
        "    rgb = rgb.transpose(2, 0, 1)\n",
        "\n",
        "    return np.uint8(rgb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "We proceed to make the `intersection-v0` environment and wrap it with the previously built wrappers. The environment is additionally wrapped with `VecFrameStack`, from Stable Baselines3 library, to give a sense of time to the policy network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "from stable_baselines3.common.env_checker import check_env\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecMonitor\n",
        "\n",
        "def make_env(config: Dict[str, Any]) -> gym.Env:\n",
        "    # Create environment\n",
        "    env = gym.make(\n",
        "        \"smarts.env:intersection-v0\",\n",
        "        headless=True,\n",
        "        visdom=False,\n",
        "        sumo_headless=True,\n",
        "        img_meters=config[\"img_meters\"],\n",
        "        img_pixels=config[\"img_pixels\"],\n",
        "    )\n",
        "\n",
        "    # Wrap env with action, reward, and observation wrapper\n",
        "    env = Info(env=env)\n",
        "    env = Action(env=env)\n",
        "    env = Reward(env=env)\n",
        "    env = Observation(env=env)\n",
        "\n",
        "    # Check custom environment\n",
        "    check_env(env)\n",
        "\n",
        "    # Wrap env with SB3 wrappers\n",
        "    env = DummyVecEnv([lambda: env])\n",
        "    env = VecFrameStack(venv=env, n_stack=config[\"n_stack\"], channels_order=\"first\")\n",
        "    env = VecMonitor(\n",
        "        venv=env,\n",
        "        filename=str(config[\"logdir\"]),\n",
        "        info_keywords=(\"is_success\",),\n",
        "    )\n",
        "\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3v0xcpPfv6L3"
      },
      "outputs": [],
      "source": [
        "The training and evaluation code is prepared using the Stable Baseline3 API. Training progress is checkpointed using a callback. At the end, the trained agent is saved and evaluated. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "sb3_example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
