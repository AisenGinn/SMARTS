smarts:
  # Environment
  visdom: False # If True, enables Visdom display.
  sumo_gui: False # If True, enables sumo-gui display.
  img_meters: 50 # Observation image area size in meters.
  img_pixels: 128 # Observation image size in pixels.
  num_stack: 3 # Number of frames to stack as input to policy network.
  action_wrapper: Discrete # Either "Lane" or "Continuous" or "Discrete"

  # Training and evaluation
  initial_collect_steps: 100
  collect_steps_per_iteration: 10

  checkpoint_freq: 1e5 # Save a model every checkpoint_freq calls to env.step().
  eval_eps: 100 # Number of evaluation epsiodes.
  eval_freq: 1e5 # Evaluate the trained model every eval_freq steps and save the best model.

  # RL algorithm
  network: q_net
  agent: dqn
  agent_kwargs:
    learning_rate: 1e-3
    target_update_period: 10 # Train step interval at which the target network is updated
    epsilon_greedy:
      initial: 1
      decay_steps: 3_000_000
      end: 0.01
  driver_steps:  
  buffer: uniform_replay
  buffer_kwargs:
    max_length: 100_000 
  # buffer: hashed_replay
  # buffer_kwargs:
  #   capacity: 500_000
  # buffer: reverb_replay
  # buffer_kwargs:
  #   replay_buffer_max_length: 100_000 

  num_iterations: 20000 # @param {type:"integer"}
  initial_collect_steps: 100  # @param {type:"integer"}
  collect_steps_per_iteration: 1 # @param {type:"integer"}
  batch_size: 64  # @param {type:"integer"}
  log_interval: 200  # @param {type:"integer"}
  num_eval_episodes: 10  # @param {type:"integer"}
  eval_interval: 1000  # @param {type:"integer"}
    
