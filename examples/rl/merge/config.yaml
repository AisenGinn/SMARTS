smarts:
  # Environment
  visdom: False # If True, enables Visdom display.
  sumo_gui: False # If True, enables sumo-gui display.
  img_meters: 50 # Observation image area size in meters.
  img_pixels: 128 # Observation image size in pixels.
  num_stack: 3 # Number of frames to stack as input to policy network.
  action_wrapper: Discrete # Either "Lane" or "Continuous" or "Discrete"

  # Training and evaluation
  train:
    iterations: 10 # 1e8  # Number of training iteration.
  driver:
    initial_collect_steps: 100
    num_steps: 100 # Collect `num_steps` steps from env for each training iteration.
  dataset:
    batch_size: 64 # Train batch size.
  num_eval_episodes: 10  
  eval_interval: 1000 
  log_interval: 200  

  # RL algorithm
  network: q_net
  agent: dqn
  agent_kwargs:
    learning_rate: 1e-3
    target_update_period: 10 # Train step interval at which the target network is updated
    n_step_update: 1
    epsilon_greedy:
      initial: 1
      decay_steps: 300_000 # Train step interval over which the epsilon greedy decays
      end: 0.01
  driver_steps:  
  buffer: uniform_replay
  buffer_kwargs:
    max_length: 100_000 
  # buffer: hashed_replay
  # buffer_kwargs:
  #   capacity: 500_000
  # buffer: reverb_replay
  # buffer_kwargs:
  #   replay_buffer_max_length: 100_000 
