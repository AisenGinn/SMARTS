smarts:
  # Environment
  visdom: False # If True, enables Visdom display.
  sumo_gui: False # If True, enables sumo-gui display.
  img_meters: 50 # Observation image area size in meters.
  img_pixels: 128 # Observation image size in pixels.
  num_stack: 4 # Number of frames to stack as input to policy network.
  action_space: Continuous
  action_wrapper: Discrete # Either "Lane" or "Continuous" or "Discrete".

  # Training and evaluation
  train_iterations: 100_000_000 # Recomended=1e8. Number of training iteration.
  log_interval: 100 # Recomended=1e2. In terms of number of training iteration.
  checkpoint_interval: 1_000 # Recomended=1e3. In terms of number of training iteration.
  driver:
    initial_steps: 100
    num_steps: 1 # Default=1. Collect `num_steps` steps from env for each iteration.
  dataset:
    batch_size: 32 # Train batch size.
  eval:
    episodes: 30  
    interval: 10_000 # Recomended=1e4. 

  # RL algorithm
  network: q_net
  agent: dqn
  agent_kwargs:
    learning_rate: 1e-3
    target_update_period: 10 # Default=1. Train step interval at which the target network is updated.
    n_step_update: 1 # Default=1. Number of steps to consider when computing TD error and TD loss in DqnAgent.
    epsilon_greedy:
      initial: 1.0
      decay_steps: 300_000 # Train step interval over which the epsilon greedy decays.
      end: 0.01
  buffer: uniform_replay
  buffer_kwargs:
    max_length: 200_000 
  # buffer: hashed_replay
  # buffer_kwargs:
  #   capacity: 100_000
  # buffer: reverb_replay
  # buffer_kwargs:
  #   replay_buffer_max_length: 100_000 
