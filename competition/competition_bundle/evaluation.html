<h2>Evaluation</h2>

<h3>Track-1: Online Learning</h3>
(a) Submitted models will be evaluated on scenarios similar to those given for training, namely, intersection, merging, cruising, and cut-in scenarios.<br>
(b) Evaluation code is available at:
    <pre><code class="bash">
    $ git clone https://github.com/huawei-noah/SMARTS.git
    $ git checkout comp-1
    $ cd <path>/SMARTS/competition/evaluation    
    </code></pre>
(c) Submitted models are scored on four aspects, namely,<br>
    <ul>
    <li>Completion: Number of goals completed.</li>
    <li>Time: Number of steps taken to complete the scenarios.</li>
    <li>Humanness: Similarity to human behaviour.</li>
    <li>Rules: Compliance with traffic rules.</li>
    </ul>
(d) Each score component must be minimized. The lower the value, the better it is.<br>
(e) Overall rank is obtained by sorting each score component in ascending order, with a priority order of Completion > Time > Humanness > Rules .<br>
(f) For more information, see <code>/SMARTS/competition/evaluation/README.md</code></li>

<h3>Track-2: Offline Learning</h3>
(a)